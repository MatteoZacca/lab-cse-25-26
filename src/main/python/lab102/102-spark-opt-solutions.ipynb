{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": [
    "# 102 Spark optimizations\n",
    "\n",
    "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
    "\n",
    "- Scala\n",
    "    - [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "    - [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "    - [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)\n",
    "- Python\n",
    "    - [Spark programming guide](https://spark.apache.org/docs/3.5.0/rdd-programming-guide.html)\n",
    "    - [All RDD APIs](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.RDD.html)\n",
    "\n",
    "Use `Tab` for autocompletion, `Shift+Tab` for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:34:50.752064Z",
     "start_time": "2024-10-20T16:34:42.694703Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# .master(\"local[N]\") <-- ask for N cores on the driver\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"Local Spark\") \\\n",
    ".config('spark.ui.port', '4040') \\\n",
    "  .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e11e83-fe01-418f-bb14-f71f6c7c8d64",
   "metadata": {},
   "source": [
    "## The weather dataset\n",
    "\n",
    "Download the following ZIP files and unzip them inside the \"datasets/big\" folder of this repo (which is not committed).\n",
    "- [weather-sample1.txt](https://big.csr.unibo.it/downloads/bigdata/weather-datasets-s1.zip) <-- start from this!\n",
    "- [weather-sample10.txt](https://big.csr.unibo.it/downloads/bigdata/weather-datasets-s10.zip)\n",
    "- [weather-full.txt](https://big.csr.unibo.it/downloads/bigdata/weather-datasets-full.zip)\n",
    "  \n",
    "The weather datasets are textual files with weather data from all over the world in year 2000 (collected from the [National Climatic Data Center](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/) of the USA. The full one weighs 13GB, the other are samples of 10% (1.3GB) and 1% (130MB) respectively.\n",
    "  - Sample row: 005733213099999**19580101**03004+51317+028783FM-12+017199999V0203201N00721004501CN0100001N9 **-0021**1-01391102681\n",
    "  - The date in YYYYMMDD format is located at 0-based position 15-23\n",
    "  - The temperatue in x10 Celsius degrees is located at 0-based positions 87-92\n",
    "\n",
    "In the dataset folder you also have *weather-stations.csv*; it is a structured file with the description of weather stations collecting the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:52.972776Z",
     "start_time": "2024-10-20T16:35:52.357262Z"
    },
    "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "def parseWeather(row):\n",
    "    usaf = row[4:10]\n",
    "    wban = row[10:15]\n",
    "    year = row[15:19]\n",
    "    month = row[19:21]\n",
    "    day = row[21:23]\n",
    "    airTemperature = row[87:92]\n",
    "    airTemperatureQuality = row[92]\n",
    "\n",
    "    return (usaf,wban,year,month,day,int(airTemperature)/10,airTemperatureQuality == '1')\n",
    "\n",
    "# STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end) \n",
    "def parseStation(row):\n",
    "    def getDouble(str):\n",
    "        return 0 if len(str)==0 else float(str)\n",
    "    \n",
    "    columns = [ x.replace(\"\\\"\",\"\") for x in row.split(\",\") ]\n",
    "    latitude = getDouble(columns[6])\n",
    "    longitude = getDouble(columns[7])\n",
    "    elevation = getDouble(columns[8])\n",
    "    return (columns[0],columns[1],columns[2],columns[3],columns[4],latitude,longitude,elevation,columns[9],columns[10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:57.630942Z",
     "start_time": "2024-10-20T16:35:56.554809Z"
    }
   },
   "outputs": [],
   "source": [
    "rddWeather = sc.\\\n",
    "  textFile(\"../../../../datasets/big/weather-sample1.txt\").\\\n",
    "  map(lambda x: parseWeather(x))\n",
    "rddStation = sc.\\\n",
    "  textFile(\"../../../../datasets/weather-stations.csv\").\\\n",
    "  map(lambda x: parseStation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
   "metadata": {
    "id": "ef4b49ee-6852-4025-9e55-3950ff937680"
   },
   "source": [
    "## 102-1 Simple job optimization\n",
    "\n",
    "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by enforcing a partitioning criteria.\n",
    "- There are multiple methods to repartition an RDD: check the ```coalesce```, ```partitionBy```, and ```repartition``` methods on the documentation and choose the best one.\n",
    "- Verify your persisted data in the web UI\n",
    "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:10:42.682093Z",
     "start_time": "2024-10-20T15:10:41.849647Z"
    },
    "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average temperature for every month\n",
    "rddWeather.\\\n",
    "  filter(lambda x: x[5]<999).\\\n",
    "  map(lambda x: (x[3], (x[5],1))).\\\n",
    "  reduceByKey(lambda v1, v2: (v1[0]+v2[0], v1[1]+v2[1])).\\\n",
    "  mapValues(lambda v: round(v[0]/v[1],2)).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614d5393d1a1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:11:13.188402Z",
     "start_time": "2024-10-20T15:11:12.853137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum temperature for every month\n",
    "rddWeather.\\\n",
    "  filter(lambda x: x[5]<999).\\\n",
    "  map(lambda x: (x[3], (x[5],1))).\\\n",
    "  reduceByKey(lambda x, y: y if x<y else x).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f9903-62cd-40a2-adc7-d821e924ea08",
   "metadata": {
    "id": "ee3f9903-62cd-40a2-adc7-d821e924ea08",
    "tags": []
   },
   "source": [
    "### Solution\n",
    "\n",
    "Caching:\n",
    "- It is good to cache the RDD **after** the common transformations (including the partitioning) have been carried out\n",
    "- Then, remember to reference the cached RDD\n",
    "- Check cached RDD in the Spark UI\n",
    "\n",
    "Partitioning:\n",
    "- ```coalesce``` aggregates partitions without shuffling; ```repartition``` and ```partitionBy``` force a shuffle\n",
    "- Differently from ```partitionBy```, ```coalesce``` and ```repartition``` do not associate a partitioning criteria to the RDD; thus, both jobs must reshuffle the cached data\n",
    "- After the aggregation, data can be reduced to a single partition (only 12 rows, one per month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841b168-df48-4e00-bd6a-6160b0838bc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:44:13.170818Z",
     "start_time": "2024-10-20T15:44:13.016924Z"
    },
    "id": "5841b168-df48-4e00-bd6a-6160b0838bc1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cachedRdd = rddWeather.\\\n",
    "  filter(lambda x: x[5]<999).\\\n",
    "  map(lambda x: (x[3], (x[5],1))).\\\n",
    "  partitionBy(8).\\\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39309a563c13e0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:44:27.441443Z",
     "start_time": "2024-10-20T15:44:26.737831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Average temperature for every month\n",
    "cachedRdd.\\\n",
    "  reduceByKey(lambda v1, v2: (v1[0]+v2[0], v1[1]+v2[1]),1).\\\n",
    "  mapValues(lambda v: round(v[0]/v[1],2)).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e614c67fc280f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:44:28.893796Z",
     "start_time": "2024-10-20T15:44:28.657879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum temperature for every month\n",
    "cachedRdd.\\\n",
    "  reduceByKey(lambda x, y: min(x,y),1).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fbf30-f568-413c-9238-de139db23135",
   "metadata": {
    "id": "377fbf30-f568-413c-9238-de139db23135"
   },
   "source": [
    "## 102-2 RDD preparation\n",
    "\n",
    "Check the five possibilities to prepare the Station RDD for subsequent processing and identify the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:51:59.650807Z",
     "start_time": "2024-10-20T15:51:59.467316Z"
    },
    "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_partitions = 8\n",
    "\n",
    "# [0] and [1] are the fields composing the key; [3] and [7] are country and elevation, respectively\n",
    "rddS1 = rddStation.\\\n",
    "  keyBy(lambda x: x[0] + x[1]).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  cache().\\\n",
    "  map(lambda kv: (kv[0],(kv[1][3],kv[1][7])))\n",
    "rddS2 = rddStation.\\\n",
    "  keyBy(lambda x: x[0] + x[1]).\\\n",
    "  map(lambda kv: (kv[0],(kv[1][3],kv[1][7]))).\\\n",
    "  cache().\\\n",
    "  partitionBy(num_partitions)\n",
    "rddS3 = rddStation.\\\n",
    "  keyBy(lambda x: x[0] + x[1]).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  map(lambda kv: (kv[0],(kv[1][3],kv[1][7]))).\\\n",
    "  cache()\n",
    "rddS4 = rddStation.\\\n",
    "  keyBy(lambda x: x[0] + x[1]).\\\n",
    "  map(lambda kv: (kv[0],(kv[1][3],kv[1][7]))).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  cache()\n",
    "rddS5 = rddStation.\\\n",
    "  map(lambda x: (x[0] + x[1], (x[3],x[7]))).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97789d-9b91-4d68-a1df-f84e7dccc759",
   "metadata": {
    "id": "0e97789d-9b91-4d68-a1df-f84e7dccc759"
   },
   "source": [
    "### Solution\n",
    "\n",
    "- keyBy() and map() break the partitioning, thus they must be issued before partitionBy()\n",
    "- Anything that happens after cache() is not saved and must be recomputed each time;\n",
    "  thus, it is good practice to cache() as later as possible\n",
    "- rddS4 and rddS5 are the best options (the latter being less verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a093dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use this to check the partitioner associated with an RDD\n",
    "print(rddS1.partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
   "metadata": {
    "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed"
   },
   "source": [
    "## 102-3 Joining RDDs\n",
    "\n",
    "Define the join between rddWeather and rddStation and compute:\n",
    "- The maximum temperature for every city\n",
    "- The maximum temperature for every city in the UK: \n",
    "  - ```StationData.country == \"UK\"```\n",
    "- Sort the results by descending temperature\n",
    "  - ```map(lambda kv: (kv[1],kv[0]))``` to invert key with value and vice versa\n",
    "\n",
    "Hints & considerations:\n",
    "- Keep only temperature values <999\n",
    "- Join syntax: ```rdd1.join(rdd2)```\n",
    "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
    "- Consider partitioning and caching to optimize the join\n",
    "  - [Scala only] Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner! To create a partitioning function, you must ```import org.apache.spark.HashPartitioner``` and then define ```p = new HashPartitioner(n)``` where ```n``` is the number of partitions to create.\n",
    "- Verify the execution plan of the join in the web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae6820-fd26-4be3-aa63-683823a6b0a3",
   "metadata": {
    "id": "e8ae6820-fd26-4be3-aa63-683823a6b0a3"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7b233",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clear the cache\n",
    "for (id, rdd) in sc._jsc.getPersistentRDDs().items():         \n",
    "    rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbfb6fb-1c5d-450a-b0d3-42c2be226fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:06:03.175764Z",
     "start_time": "2024-10-20T16:06:00.478094Z"
    },
    "id": "7fbfb6fb-1c5d-450a-b0d3-42c2be226fad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First version: caching the join\n",
    "num_partitions = 8\n",
    "\n",
    "rddS = rddStation.\\\n",
    "  map(lambda s: (s[0]+s[1], (s[2], s[3]))).\\\n",
    "rddW = rddWeather.\\\n",
    "  filter(lambda w: w[6]).\\\n",
    "  map(lambda w: (w[0]+w[1], w[5]))\n",
    "\n",
    "rddJoin = rddW.join(rddS,num_partitions).cache()\n",
    "\n",
    "rddJoin.\\\n",
    "  map(lambda j: (j[1][1][0], j[1][0])).\\\n",
    "  reduceByKey(lambda x,y: min(x,y)).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d021d55e5b45dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:06:49.284408Z",
     "start_time": "2024-10-20T16:06:49.000634Z"
    }
   },
   "outputs": [],
   "source": [
    "rddJoin.\\\n",
    "  filter(lambda j: j[1][1][1] == \"UK\").\\\n",
    "  map(lambda j: (j[1][1][0], j[1][0])).\\\n",
    "  reduceByKey(lambda x,y: min(x,y)).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a26359d3cdc817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:07:16.818733Z",
     "start_time": "2024-10-20T16:07:16.302831Z"
    }
   },
   "outputs": [],
   "source": [
    "rddJoin.\\\n",
    "  filter(lambda j: j[1][1][1] == \"UK\").\\\n",
    "  map(lambda j: (j[1][1][0], j[1][0])).\\\n",
    "  reduceByKey(lambda x,y: min(x,y)).\\\n",
    "  map(lambda kv: (kv[1],kv[0])).\\\n",
    "  sortByKey(False).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468f815-5aba-48d8-9105-9c185b2c6a41",
   "metadata": {},
   "source": [
    "Comments:\n",
    "- The first job is the one taking longer because it has to read everything from disk; the other two will be faster because they rely on the cached results\n",
    "- In PySpark, the rddJoin may be internally cached by Spark even if the method is not explicitly called; since this behavior is not predictable, always declare caching when needed\n",
    "- The sortByKey is implemented by Spark as an action (even though, theoretically, it is just a transformation); this leads to a fragmentation of the third job, that (in the UI) gets split into multiple jobs\n",
    "- You will notice that Spark subdivides the input into splits of 32MB; in the WebUI you will see that:\n",
    "    - In the first stage, partitions are quite uniform (no skew in the data)\n",
    "    - In the second stage (after the shuffle for the join), the data remains well distributed, but with only few records in each partition. Thus, it could make sense to reduce the number of partitions during the join (to avoid having too many tasks), even though the time reduction is not significant (in this case)\n",
    "- Further optimization can be achieved by caching the result of the aggregation, which is used by all the three jobs. Notice that, to be able to filter by country, the reduceByKey must include the country in the key; on the one hand, this means that the aggregation costs more (because instead of aggregating <city,temp> pairs, we aggregate <(city,country),temp> pairs); however, this means that the aggregation is done only once (instead og thrice), so this is ultimately much convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802d09bf8838caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:09:20.510836Z",
     "start_time": "2024-10-20T16:09:18.773149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Second version: caching the aggregation\n",
    "num_partitions = 8\n",
    "\n",
    "rddS = rddStation.\\\n",
    "  map(lambda s: (s[0]+s[1], (s[2], s[3]))).\\\n",
    "  partitionBy(num_partitions)\n",
    "rddW = rddWeather.\\\n",
    "  filter(lambda w: w[6]).\\\n",
    "  map(lambda w: (w[0]+w[1], w[5])).\\\n",
    "  partitionBy(num_partitions)\n",
    "\n",
    "rddAgg = rddW.\\\n",
    "  join(rddS).\\\n",
    "  map(lambda j: ((j[1][1][0],j[1][1][1]), j[1][0])).\\\n",
    "  reduceByKey(lambda x,y: min(x,y)).\\\n",
    "  cache()\n",
    "\n",
    "rddAgg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334e4eca061b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:09:29.350028Z",
     "start_time": "2024-10-20T16:09:29.172805Z"
    }
   },
   "outputs": [],
   "source": [
    "rddAgg.\\\n",
    "  filter(lambda a: a[0][1]==\"UK\").\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fb4461de62101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:10:02.554266Z",
     "start_time": "2024-10-20T16:10:02.297151Z"
    }
   },
   "outputs": [],
   "source": [
    "rddAgg.\\\n",
    "  filter(lambda a: a[0][1]==\"UK\").\\\n",
    "  map(lambda kv: (kv[1],kv[0])).\\\n",
    "  sortByKey(False).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
   "metadata": {
    "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882"
   },
   "source": [
    "## 102-4 Memory occupation\n",
    "\n",
    "Use Spark's web UI to verify the space occupied by the provided RDDs.\n",
    "\n",
    "*Warning*: in PySpark, StoraleLevels use serialization by default (see [documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:12:02.380987Z",
     "start_time": "2024-10-20T16:12:02.234579Z"
    },
    "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Clear the cache\n",
    "for (id, rdd) in sc._jsc.getPersistentRDDs().items():         \n",
    "    rdd.unpersist()\n",
    "\n",
    "memSerRdd = rddWeather.cache()\n",
    "memRdd = memSerRdd.map(lambda x: x).persist(StorageLevel.MEMORY_AND_DISK_DESER)\n",
    "diskRdd = memSerRdd.map(lambda x: x).persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce473ab4-b676-4ae3-b7e6-a4c61f1b3c3e",
   "metadata": {
    "id": "ce473ab4-b676-4ae3-b7e6-a4c61f1b3c3e"
   },
   "source": [
    "### Solution\n",
    "\n",
    "- Collecting/Saving to file is required to trigger the (lazy) evaluation\n",
    "- memSerRdd's size will be less than half of memRdd's\n",
    "- diskRdd's size will be approximately the same as memSerRdd (output to disk is always serialized)\n",
    "- StorageLevel differ in Scala and Python:\n",
    "    - In Scala, `cache()` is deserialized and you have `*_SER` StorageLevels for the serialized versions\n",
    "    - In PySpark, `cache()` is serialized and you have `*_DESER` StorageLevels for the deserialized versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404753f7-1dd6-4997-a0af-9d43f3e9c4ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:12:07.010670Z",
     "start_time": "2024-10-20T16:12:05.066502Z"
    },
    "id": "404753f7-1dd6-4997-a0af-9d43f3e9c4ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "memSerRdd.collect()\n",
    "memRdd.collect()\n",
    "diskRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271ded5-a114-40a0-b2d3-5baf13ce2741",
   "metadata": {},
   "source": [
    "## 102-5 Evaluating different join methods\n",
    "\n",
    "Consider the following scenario:\n",
    "- We have a disposable RDD of Weather data (i.e., it is used only once): ```rddW```\n",
    "- And we have an RDD of Station data that is used many times: ```rddS```\n",
    "- Both RDDs are cached (```collect()```is called to enforce caching)\n",
    "\n",
    "We want to join the two RDDS. Which option is best?\n",
    "- Simply join the two RDDs\n",
    "- Enforce on ```rddW1``` the same partitioner of ```rddS``` (and then join)\n",
    "- Exploit broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4f4d0-9437-4bc2-b401-6f986a663e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 8\n",
    "\n",
    "rddW = rddWeather.\\\n",
    "  filter(lambda w: w[5]<999).\\\n",
    "  keyBy(lambda w: w[0]+w[1]).\\\n",
    "  cache()\n",
    "\n",
    "rddS = rddStation.\\\n",
    "  keyBy(lambda s: s[0]+s[1]).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  cache()\n",
    "\n",
    "# Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed99e7-a94e-44da-bf81-8750a1584eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it better to simply join the two RDDs..\n",
    "rddX = rddW.\\\n",
    "  join(rddS).\\\n",
    "  map(lambda kv: (kv[1][1][2],kv[1][0][5])).\\\n",
    "  reduceByKey(lambda x,y: min(x,y),1)\n",
    "print(rddX.toDebugString().decode(\"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669bb7e-3990-4877-aa8f-74d2b6ed5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddX.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19a656-458b-419f-8c18-a6e559ce8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..to enforce on rddW1 the same partitioner of rddS..\n",
    "rddX = rddW.\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  join(rddS).\\\n",
    "  map(lambda kv: (kv[1][1][2],kv[1][0][5])).\\\n",
    "  reduceByKey(lambda x,y: min(x,y),1)\n",
    "print(rddX.toDebugString().decode(\"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46187f-dccf-41df-ab3b-e24828f0fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddX.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd9c96-c886-45f8-be03-6bda6f075cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..or to exploit broadcast variables?\n",
    "bRddS = sc.broadcast(rddS.map(lambda s: (s[0], s[1][2])).collectAsMap())\n",
    "rddJ = rddW.\\\n",
    "  map(lambda kv: (bRddS.value.get(kv[0]), kv[1][5])).\\\n",
    "  filter(lambda x: x[0] is not None)\n",
    "\n",
    "rddX = rddJ.\\\n",
    "  reduceByKey(lambda x,y: min(x,y),1)\n",
    "print(rddX.toDebugString().decode(\"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ddf84c-16b0-4e04-8134-3b1eb233cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddX.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa056205-70a5-4929-9cae-7188a21cba9e",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "- Enforcing a partitioner doesn't make sense if the rdd is used in one shuffle operation only: the partitioning will be done directly during that shuffle\n",
    "- Broadcasting can be very powerful to optimize joins, BUT only if the dataset is small enough to easily fit the memory of the executors; if the dataset is bigger, it should be an RDD"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
