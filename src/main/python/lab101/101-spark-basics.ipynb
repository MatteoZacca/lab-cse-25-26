{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd7243a-b7d3-4e47-b3ec-49cafdebada5",
   "metadata": {},
   "source": [
    "# 101 Spark basics\n",
    "\n",
    "The goal of this lab is to get familiar with Spark programming.\n",
    "\n",
    "- Scala\n",
    "    - [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "    - [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "    - [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)\n",
    "- Python\n",
    "    - [Spark programming guide](https://spark.apache.org/docs/3.5.0/rdd-programming-guide.html)\n",
    "    - [All RDD APIs](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.RDD.html)\n",
    "\n",
    "Use `Tab` for autocompletion, `Shift+Tab` for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bb4f39692d0432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T13:12:01.893550Z",
     "start_time": "2024-10-20T13:11:46.380948Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e281923de95ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f33cb070331b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Local Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Local Spark>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a local Spark session (running on your machine, not a cluster). \n",
    "# .master(\"local\"): tells Spark to run locally using your CPU cores4\n",
    "# .appName(\"Local Spark\"): sets the name that appears in the Spark UI\n",
    "# \"local[*]\" if you want to use all your lcoal cores \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Local Spark\") \\\n",
    "    .config('spark.ui.port', '4040') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Gets the SparkContext, the low-level API that manages RDDs\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce2123-0810-4e95-8930-c8763b913fc2",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3bc0d93-b388-4d14-9ae0-ca4ebbd892f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6], [7, 8]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc.parallelize() takes a Python collection (list, range, ect.) and converts \n",
    "# it into a Spark RDD.\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "rdd = sc.parallelize(data, numSlices = 4) # Each partition is a chunk of data \n",
    "# that a task will process.\n",
    "rdd.glom().collect()\n",
    "\n",
    "# parallelize() is mainly for small datasets in memory. For large datasets, \n",
    "# it’s better to use sc.textFile or DataFrames.\n",
    "# The number of partitions controls parallelism, not the total number of \n",
    "# elements.\n",
    "# Use .getNumPartitions() to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d397b27d-4d82-4395-894b-bd36bc33dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wholeTextFiles keeps the entire file content together, and each element is a\n",
    "# tuple (file_path, file_content)\n",
    "rdd = sc.wholeTextFiles(\"../../../../datasets\")\n",
    "#rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a0be28e5e823",
   "metadata": {},
   "source": [
    "## 101-1 Spark warm-up\n",
    "\n",
    "Load the ```capra``` and ```divinacommedia``` datasets and try the following actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc4b5cee4a93755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T13:25:49.644422Z",
     "start_time": "2024-10-20T13:25:49.169096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cores:  1\n"
     ]
    }
   ],
   "source": [
    "num_cores = sc.defaultParallelism\n",
    "print(\"Available cores: \", num_cores)\n",
    "rddCapra = sc.textFile(\"../../../../datasets/capra.txt\", minPartitions = num_cores)\n",
    "rddDC = sc.textFile(\"../../../../datasets/divinacommedia.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e60ece0-e2f3-4e29-a7b3-462af9e1bc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the file is stored on the local filesystem (inside Docker) - not HDFS -\n",
    "# Spark cannot split it into multiple 128 MB blocks automatically\n",
    "rddCapra.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65223cbb-90c7-4b69-824c-3c88ad0a02fc",
   "metadata": {},
   "source": [
    "- Show their content (```collect```)\n",
    "- Count their rows (```count```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0530cc59-ade1-4ba7-9709-8ab734067003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sopra la panca la capra campa', 'sotto la panca la capra crepa']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCapra.collect()\n",
    "# rddCapra.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c7c2b-c18f-4657-add8-73479e56cf47",
   "metadata": {},
   "source": [
    "- Split phrases into words (```map``` or ```flatMap```; what’s the difference?)\n",
    "- Check the results (remember: evaluation is lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963627de-73ce-45e4-b384-8efea2a67b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is lazy evaluation in Spark?\n",
    "# In Spark, transformations like map() and flatMap() are lazy. \n",
    "# Lazy means: Spark does not execute them immediately when you define them.\n",
    "# Spark just builds a plan (DAG) describing how to compute the RDD.\n",
    "# The actual computations is triggered only when you call an action (collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6699f560-7272-437b-8ba9-b5e8a508b621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sopra', 'la', 'panca', 'la', 'capra', 'campa'],\n",
       " ['sotto', 'la', 'panca', 'la', 'capra', 'crepa']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCapraWords1 = rddCapra.map(lambda element : element.split(\" \"))\n",
    "rddCapraWords1.collect()\n",
    "# with map each element of the new RDD is a list of words, but nested structure\n",
    "# remains (list inside RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce0207a-c6e1-491c-b0a1-63123e2d375d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCapraWords1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8a2011-6f2f-4215-afe2-19ce7e912eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sopra',\n",
       " 'la',\n",
       " 'panca',\n",
       " 'la',\n",
       " 'capra',\n",
       " 'campa',\n",
       " 'sotto',\n",
       " 'la',\n",
       " 'panca',\n",
       " 'la',\n",
       " 'capra',\n",
       " 'crepa']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCapraWords2 = rddCapra.flatMap(lambda element : element.split(\" \"))\n",
    "rddCapraWords2.collect()\n",
    "# flatMap() flattens all the lists into a single RDD of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5ca7e2c-c0f1-454c-ba0f-3305aaddc75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCapraWords2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861245c-17cd-477d-a25d-562fdd5eaea4",
   "metadata": {},
   "source": [
    "- Try the ```toDebugString``` function to check the execution plan\n",
    "    - In PySpark, use ```toDebugString().decode(\"unicode_escape\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d280a51c-f1a8-4043-ba91-af1f3c8bdd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) PythonRDD[16] at collect at /tmp/ipykernel_249/3570472243.py:11 []\n",
      " |  MapPartitionsRDD[15] at mapPartitions at PythonRDD.scala:160 []\n",
      " |  ShuffledRDD[14] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(1) PairwiseRDD[13] at reduceByKey at /tmp/ipykernel_249/3570472243.py:10 []\n",
      "    |  PythonRDD[12] at reduceByKey at /tmp/ipykernel_249/3570472243.py:10 []\n",
      "    |  ../../../../datasets/capra.txt MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  ../../../../datasets/capra.txt HadoopRDD[4] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "# Every RDD in Spark keeps track of its lineage, i.e., the chain of \n",
    "# transformations that led to it. \n",
    "# toDebugString prints a textual representation of this DAG (Directed Acyclic \n",
    "# Graph). It's useful for debugging, understanding lazy evaluation, and seeing\n",
    "# how many stages and partitions Spark will use.\n",
    "\n",
    "rddL = rddCapra. \\\n",
    "   flatMap(lambda x : x.split(\" \") ). \\\n",
    "   map(lambda x : (x,1)). \\\n",
    "   reduceByKey(lambda x,y : x+y)\n",
    "rddL.collect()\n",
    "print(rddL.toDebugString().decode(\"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d04659ace472d8",
   "metadata": {},
   "source": [
    "## 101-2 Basic Spark jobs\n",
    "\n",
    "Implement on Spark the following jobs and test them on both capra and divinacommedia datasets.\n",
    "\n",
    "- **Word count**: count the number of occurrences of each word\n",
    "  - Result: (sopra, 1), (la, 4), …\n",
    "- **Word length count**: count the number of occurrences of words of given lengths\n",
    "  - Result: (2, 4), (5, 8)\n",
    "- Count the **average length of words given their first letter** (i.e., words that begin with \"s\" have an average length of 5)\n",
    "  - Result: (s, 5), (l, 2), …\n",
    "- Return the **inverted index of words** (i.e., for each word, list the numbers of lines in which they appear)\n",
    "  - Result: (sopra, (0)), (la, (0, 1)), ...\n",
    "\n",
    "Also, check how sorting works and try to sort key-value RDDs by descending values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e51bcec0-5822-4df2-889f-94a0a1bb25fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'la'),\n",
       " (2, 'panca'),\n",
       " (2, 'capra'),\n",
       " (1, 'sopra'),\n",
       " (1, 'campa'),\n",
       " (1, 'sotto'),\n",
       " (1, 'crepa')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count\n",
    "rddCapra. \\\n",
    "    flatMap(lambda x : x.split(\" \")). \\\n",
    "    map(lambda x : (x,1)). \\\n",
    "    reduceByKey(lambda x,y : x + y). \\\n",
    "    map(lambda kv : (kv[1], kv[0])). \\\n",
    "    sortByKey(False). \\\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a57d9e0b-6724-4bd4-b68f-8623194512d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 8), (2, 4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word length count\n",
    "rddCapra. \\\n",
    "    flatMap(lambda x : x.split(\" \")). \\\n",
    "    map(lambda x : (len(x), 1)). \\\n",
    "    reduceByKey(lambda x,y : x + y). \\\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b819e1e-6a8c-4fc8-8040-6d40e939539b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 5.0), ('l', 2.0), ('p', 5.0), ('c', 5.0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average length of words given their first letter\n",
    "rddCapra. \\\n",
    "    flatMap(lambda x : x.split(\" \")). \\\n",
    "    filter(lambda x : len(x) > 0). \\\n",
    "    map(lambda x : (x[0].lower(), (1, len(x)))). \\\n",
    "    reduceByKey(lambda x,y : (x[0] + y[0], x[1] + y[1])). \\\n",
    "    mapValues(lambda v : v[1]/v[0]). \\\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dff081f-7bf4-4945-8e04-4f9feb6db445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 5.0), ('l', 2.0), ('p', 5.0), ('c', 5.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average length of words given their first letter (alternative on the final map)\n",
    "rddCapra. \\\n",
    "    flatMap(lambda x : x.split(\" \")). \\\n",
    "    filter(lambda x : len(x) > 0). \\\n",
    "    map(lambda x : (x[0].lower(), (1, len(x)))). \\\n",
    "    reduceByKey(lambda x,y : (x[0] + y[0], x[1] + y[1])). \\\n",
    "    map(lambda kv : (kv[0], kv[1][1]/kv[1][0])). \\\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fabcb2e7-3cd0-49d6-97f5-2d83d5bf8c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sopra', (0,)),\n",
       " ('la', (0, 1)),\n",
       " ('panca', (0, 1)),\n",
       " ('capra', (0, 1)),\n",
       " ('campa', (0,)),\n",
       " ('sotto', (1,)),\n",
       " ('crepa', (1,))]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each word, list the numbers of lines in which they appear\n",
    "rddCapra. \\\n",
    "    map(lambda x : x.split(\" \")). \\\n",
    "    zipWithIndex(). \\\n",
    "    flatMap(lambda x : [(word, x[1]) for word in x[0]]). \\\n",
    "    groupByKey(). \\\n",
    "    mapValues(lambda idxs: tuple(sorted(set(idxs)))). \\\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7ef65-f9b2-4495-bfe9-6f22e34c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each word, list the numbers of lines in which they appear\n",
    "rddCapra \\\n",
    "    .map(lambda x : x.split(\" \")) \\\n",
    "    .zipWithIndex() \\\n",
    "    .map(lambda kv : (kv[1], kv[0])) \\\n",
    "    .flatMapValues(lambda v : v) \\\n",
    "    .map(lambda )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c49c3689d65e2",
   "metadata": {},
   "source": [
    "## 101-3 Extra Spark jobs\n",
    "\n",
    "Implement the following job.\n",
    "\n",
    "- **Co-occurrence count**: count the number of co-occurrences in the text. A co-occurrence is defined as \"two distinct words appearing in the same line\".\n",
    "  - In the first line of the *capra* dataset, co-occurrences are:\n",
    "     - (sopra, la), (sopra, panca), (sopra, capra), (sopra, campa)\n",
    "     - (la, sopra), (la, panca), (la, capra), (la, campa) \n",
    "     - (panca, sopra), (panca, la), (panca, capra), (panca, campa)\n",
    "     - (capra, sopra), (capra, la), (capra, panca), (capra, campa)\n",
    "     - (campa, sopra), (campa, la), (campa, panca), (campa, capra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4686a419-0f72-4330-a6e3-4f6df07b8218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('capra', 'panca'),\n",
       "  ('capra', 'campa'),\n",
       "  ('capra', 'la'),\n",
       "  ('capra', 'sopra'),\n",
       "  ('panca', 'capra'),\n",
       "  ('panca', 'campa'),\n",
       "  ('panca', 'la'),\n",
       "  ('panca', 'sopra'),\n",
       "  ('campa', 'capra'),\n",
       "  ('campa', 'panca'),\n",
       "  ('campa', 'la'),\n",
       "  ('campa', 'sopra'),\n",
       "  ('la', 'capra'),\n",
       "  ('la', 'panca'),\n",
       "  ('la', 'campa'),\n",
       "  ('la', 'sopra'),\n",
       "  ('sopra', 'capra'),\n",
       "  ('sopra', 'panca'),\n",
       "  ('sopra', 'campa'),\n",
       "  ('sopra', 'la')],\n",
       " [('capra', 'panca'),\n",
       "  ('capra', 'sotto'),\n",
       "  ('capra', 'la'),\n",
       "  ('capra', 'crepa'),\n",
       "  ('panca', 'capra'),\n",
       "  ('panca', 'sotto'),\n",
       "  ('panca', 'la'),\n",
       "  ('panca', 'crepa'),\n",
       "  ('sotto', 'capra'),\n",
       "  ('sotto', 'panca'),\n",
       "  ('sotto', 'la'),\n",
       "  ('sotto', 'crepa'),\n",
       "  ('la', 'capra'),\n",
       "  ('la', 'panca'),\n",
       "  ('la', 'sotto'),\n",
       "  ('la', 'crepa'),\n",
       "  ('crepa', 'capra'),\n",
       "  ('crepa', 'panca'),\n",
       "  ('crepa', 'sotto'),\n",
       "  ('crepa', 'la')]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Co-occurrence count\n",
    "rddCapra \\\n",
    "    .map(lambda x : list(set(x.split(\" \")))) \\\n",
    "    .map(lambda words: [(words[i], words[j])\n",
    "                            for i in range(len(words))\n",
    "                            for j in range(len(words))\n",
    "                            if i != j]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d93b4-2f1f-44b7-b9d2-f54739194418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
