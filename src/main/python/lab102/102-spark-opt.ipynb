{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": [
    "# 102 Spark optimizations\n",
    "\n",
    "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
    "\n",
    "- Scala\n",
    "    - [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "    - [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "    - [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)\n",
    "- Python\n",
    "    - [Spark programming guide](https://spark.apache.org/docs/3.5.0/rdd-programming-guide.html)\n",
    "    - [All RDD APIs](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.RDD.html)\n",
    "\n",
    "Use `Tab` for autocompletion, `Shift+Tab` for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:34:50.752064Z",
     "start_time": "2024-10-20T16:34:42.694703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ea89e32ca9a0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Local Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[8] appName=Local Spark>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# .master(\"local[N]\") <-- ask for N cores on the driver\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[8]\") \\\n",
    ".appName(\"Local Spark\") \\\n",
    ".config('spark.ui.port', '4040') \\\n",
    "  .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e11e83-fe01-418f-bb14-f71f6c7c8d64",
   "metadata": {},
   "source": [
    "## The weather dataset\n",
    "\n",
    "Download the following ZIP files and unzip them inside the \"datasets/big\" folder of this repo (which is not committed).\n",
    "- [weather-sample1.txt](https://big.csr.unibo.it/downloads/bigdata/weather-datasets-s1.zip) <-- start from this!\n",
    "- [weather-sample10.txt](https://big.csr.unibo.it/downloads/bigdata/weather-datasets-s10.zip)\n",
    "- [weather-full.txt](https://big.csr.unibo.it/downloads/bigdata/weather-datasets-full.zip)\n",
    "  \n",
    "The weather datasets are textual files with weather data from all over the world in year 2000 (collected from the [National Climatic Data Center](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/) of the USA. The full one weighs 13GB, the other are samples of 10% (1.3GB) and 1% (130MB) respectively.\n",
    "  - Sample row: 005733213099999**19580101**03004+51317+028783FM-12+017199999V0203201N00721004501CN0100001N9 **-0021**1-01391102681\n",
    "  - The date in YYYYMMDD format is located at 0-based position 15-23\n",
    "  - The temperatue in x10 Celsius degrees is located at 0-based positions 87-92\n",
    "\n",
    "In the dataset folder you also have *weather-stations.csv*; it is a structured file with the description of weather stations collecting the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:52.972776Z",
     "start_time": "2024-10-20T16:35:52.357262Z"
    },
    "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "def parseWeather(row):\n",
    "    usaf = row[4:10]\n",
    "    wban = row[10:15]\n",
    "    year = row[15:19]\n",
    "    month = row[19:21]\n",
    "    day = row[21:23]\n",
    "    airTemperature = row[87:92]\n",
    "    airTemperatureQuality = row[92]\n",
    "\n",
    "    return (usaf,wban,year,month,day,int(airTemperature)/10,airTemperatureQuality == '1')\n",
    "\n",
    "# STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end) \n",
    "def parseStation(row):\n",
    "    def getDouble(str):\n",
    "        return 0 if len(str)==0 else float(str)\n",
    "    \n",
    "    columns = [ x.replace(\"\\\"\",\"\") for x in row.split(\",\") ]\n",
    "    latitude = getDouble(columns[6])\n",
    "    longitude = getDouble(columns[7])\n",
    "    elevation = getDouble(columns[8])\n",
    "    return (columns[0],columns[1],columns[2],columns[3],columns[4],latitude,longitude,elevation,columns[9],columns[10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:57.630942Z",
     "start_time": "2024-10-20T16:35:56.554809Z"
    }
   },
   "outputs": [],
   "source": [
    "rddWeather = sc.\\\n",
    "  textFile(\"../../../../datasets/big/weather-sample1.txt\") \\\n",
    "    .map(lambda x: parseWeather(x)) \\\n",
    "\n",
    "rddWeather.collect()\n",
    "\n",
    "rddStation = sc.\\\n",
    "  textFile(\"../../../../datasets/weather-stations.csv\").\\\n",
    "  map(lambda x: parseStation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
   "metadata": {
    "id": "ef4b49ee-6852-4025-9e55-3950ff937680"
   },
   "source": [
    "## 102-1 Simple job optimization\n",
    "\n",
    "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by enforcing a partitioning criteria.\n",
    "- There are multiple methods to repartition an RDD: check the ```coalesce```, ```partitionBy```, and ```repartition``` methods on the documentation and choose the best one.\n",
    "- Verify your persisted data in the web UI\n",
    "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:10:42.682093Z",
     "start_time": "2024-10-20T15:10:41.849647Z"
    },
    "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average temperature for every month\n",
    "rddWeather.\\\n",
    "  filter(lambda x: x[5]<999).\\\n",
    "  map(lambda x: (x[3], (x[5],1))).\\\n",
    "  reduceByKey(lambda v1, v2: (v1[0]+v2[0], v1[1]+v2[1])).\\\n",
    "  mapValues(lambda v: round(v[0]/v[1],2)).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614d5393d1a1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:11:13.188402Z",
     "start_time": "2024-10-20T15:11:12.853137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum temperature for every month\n",
    "rddWeather.\\\n",
    "  filter(lambda x: x[5]<999).\\\n",
    "  map(lambda x: (x[3], (x[5],1))).\\\n",
    "  reduceByKey(lambda x, y: y if x<y else x).\\\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fbf30-f568-413c-9238-de139db23135",
   "metadata": {
    "id": "377fbf30-f568-413c-9238-de139db23135"
   },
   "source": [
    "## 102-1 RDD preparation\n",
    "\n",
    "Check the five possibilities to prepare the Station RDD for subsequent processing and identify the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:51:59.650807Z",
     "start_time": "2024-10-20T15:51:59.467316Z"
    },
    "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_partitions = 8\n",
    "\n",
    "# [0] and [1] are the fields composing the key; [3] and [7] are country and elevation, respectively\n",
    "rddS1 = rddStation.\\\n",
    "  keyBy(lambda x: x[0] + x[1]).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  cache().\\\n",
    "  map(lambda kv: (kv[0],(kv[1][3],kv[1][7])))\n",
    "rddS2 = rddStation.\\\n",
    "  keyBy(lambda x: x[0] + x[1]).\\\n",
    "  map(lambda kv: (kv[0],(kv[1][3],kv[1][7]))).\\\n",
    "  cache().\\\n",
    "  partitionBy(num_partitions)\n",
    "rddS3 = rddStation.\\\n",
    "  keyBy(lambda x: x[0] + x[1]).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  map(lambda kv: (kv[0],(kv[1][3],kv[1][7]))).\\\n",
    "  cache()\n",
    "rddS4 = rddStation.\\\n",
    "  keyBy(lambda x: x[0] + x[1]).\\\n",
    "  map(lambda kv: (kv[0],(kv[1][3],kv[1][7]))).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  cache()\n",
    "rddS5 = rddStation.\\\n",
    "  map(lambda x: (x[0] + x[1], (x[3],x[7]))).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
   "metadata": {
    "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed"
   },
   "source": [
    "## 103-3 Joining RDDs\n",
    "\n",
    "Define the join between rddWeather and rddStation and compute:\n",
    "- The maximum temperature for every city\n",
    "- The maximum temperature for every city in the UK: \n",
    "  - ```StationData.country == \"UK\"```\n",
    "- Sort the results by descending temperature\n",
    "  - ```map(lambda kv: (kv[1],kv[0]))``` to invert key with value and vice versa\n",
    "\n",
    "Hints & considerations:\n",
    "- Keep only temperature values <999\n",
    "- Join syntax: ```rdd1.join(rdd2)```\n",
    "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
    "- Consider partitioning and caching to optimize the join\n",
    "  - [Scala only] Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner! To create a partitioning function, you must ```import org.apache.spark.HashPartitioner``` and then define ```p = new HashPartitioner(n)``` where ```n``` is the number of partitions to create.\n",
    "- Verify the execution plan of the join in the web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
   "metadata": {
    "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882"
   },
   "source": [
    "## 103-4 Memory occupation\n",
    "\n",
    "Use Spark's web UI to verify the space occupied by the provided RDDs.\n",
    "\n",
    "*Warning*: in PySpark, StoraleLevels use serialization by default (see [documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:12:02.380987Z",
     "start_time": "2024-10-20T16:12:02.234579Z"
    },
    "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Clear the cache\n",
    "for (id, rdd) in sc._jsc.getPersistentRDDs().items():         \n",
    "    rdd.unpersist()\n",
    "\n",
    "memSerRdd = rddWeather.cache()\n",
    "memRdd = memSerRdd.map(lambda x: x).persist(StorageLevel. MEMORY_AND_DISK_DESER)\n",
    "diskRdd = memSerRdd.map(lambda x: x).persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271ded5-a114-40a0-b2d3-5baf13ce2741",
   "metadata": {},
   "source": [
    "## 102-5 Evaluating different join methods\n",
    "\n",
    "Consider the following scenario:\n",
    "- We have a disposable RDD of Weather data (i.e., it is used only once): ```rddW```\n",
    "- And we have an RDD of Station data that is used many times: ```rddS```\n",
    "- Both RDDs are cached (```collect()```is called to enforce caching)\n",
    "\n",
    "We want to join the two RDDS. Which option is best?\n",
    "- Simply join the two RDDs\n",
    "- Enforce on ```rddW1``` the same partitioner of ```rddS``` (and then join)\n",
    "- Exploit broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4f4d0-9437-4bc2-b401-6f986a663e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 8\n",
    "\n",
    "rddW = rddWeather.\\\n",
    "  filter(lambda w: w[5]<999).\\\n",
    "  keyBy(lambda w: w[0]+w[1]).\\\n",
    "  cache()\n",
    "\n",
    "rddS = rddStation.\\\n",
    "  keyBy(lambda s: s[0]+s[1]).\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  cache()\n",
    "\n",
    "# Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed99e7-a94e-44da-bf81-8750a1584eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it better to simply join the two RDDs..\n",
    "rddX = rddW.\\\n",
    "  join(rddS).\\\n",
    "  map(lambda kv: (kv[1][1][2],kv[1][0][5])).\\\n",
    "  reduceByKey(lambda x,y: min(x,y),1)\n",
    "print(rddX.toDebugString().decode(\"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669bb7e-3990-4877-aa8f-74d2b6ed5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddX.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19a656-458b-419f-8c18-a6e559ce8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..to enforce on rddW1 the same partitioner of rddS..\n",
    "rddX = rddW.\\\n",
    "  partitionBy(num_partitions).\\\n",
    "  join(rddS).\\\n",
    "  map(lambda kv: (kv[1][1][2],kv[1][0][5])).\\\n",
    "  reduceByKey(lambda x,y: min(x,y),1)\n",
    "print(rddX.toDebugString().decode(\"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46187f-dccf-41df-ab3b-e24828f0fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddX.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd9c96-c886-45f8-be03-6bda6f075cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..or to exploit broadcast variables?\n",
    "bRddS = sc.broadcast(rddS.map(lambda s: (s[0], s[1][2])).collectAsMap())\n",
    "rddJ = rddW.\\\n",
    "  map(lambda kv: (bRddS.value.get(kv[0]), kv[1][5])).\\\n",
    "  filter(lambda x: x[0] is not None)\n",
    "\n",
    "rddX = rddJ.\\\n",
    "  reduceByKey(lambda x,y: min(x,y),1)\n",
    "print(rddX.toDebugString().decode(\"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ddf84c-16b0-4e04-8134-3b1eb233cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddX.collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
